{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhmBsOl/iRNdHKaeWUhw1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee222c952c78416993009483ddba8c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_475447bda55b409880aeb59e0d8d8d46",
              "IPY_MODEL_f521b0efc471489b8377b489b0ce510c",
              "IPY_MODEL_cf03d8386723458eb622d7df25b47a33"
            ],
            "layout": "IPY_MODEL_47d34d127fc6415d83faa9b6dd429005"
          }
        },
        "475447bda55b409880aeb59e0d8d8d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_501a611b0ac64e11a9771dbc404d7e23",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_87fed1be2e7e421aa065ebc644b96e8a",
            "value": "Loading‚Äápipeline‚Äácomponents...:‚Äá100%"
          }
        },
        "f521b0efc471489b8377b489b0ce510c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0e55919caf4961828584594a08cb43",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e92aab6f3a924fe1867d03ecdc9baef9",
            "value": 6
          }
        },
        "cf03d8386723458eb622d7df25b47a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbae7db0217341649958a63558145a74",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ff6a724d4da746aab59652042a52da1d",
            "value": "‚Äá6/6‚Äá[00:13&lt;00:00,‚Äá‚Äá4.34s/it]"
          }
        },
        "47d34d127fc6415d83faa9b6dd429005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501a611b0ac64e11a9771dbc404d7e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87fed1be2e7e421aa065ebc644b96e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e0e55919caf4961828584594a08cb43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92aab6f3a924fe1867d03ecdc9baef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbae7db0217341649958a63558145a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff6a724d4da746aab59652042a52da1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindhu402/Blue_Bean_Scholarship_Chatbot/blob/main/Major_Project_111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rrSNJaSEXeY"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio pytubefix youtube-transcript-api openai-whisper transformers torch python-dotenv requests\n",
        "\n",
        "# Install ffmpeg for audio processing\n",
        "!apt-get install -y ffmpeg -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STABLE DIFFUSION + LoRA TRAINING FOR COLAB\n",
        "# ============================================\n",
        "# Train your own LoRA model with custom dataset from Google Drive\n",
        "\n",
        "# ===== PART 1: SETUP & INSTALLATION =====\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "!pip install -q peft bitsandbytes xformers\n",
        "!pip install -q datasets opencv-python moviepy\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===== PART 2: CONFIGURE YOUR TRAINING =====\n",
        "\n",
        "# === IMPORTANT: SET YOUR PATHS HERE ===\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Image_Dataset/final\"  # Folder with your images\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/lora_output\"  # Where to save trained LoRA\n",
        "INSTANCE_PROMPT = \"A beam of white light refracting and splitting into a spectrum of colors as it passes through a triangular glass prism. Show the distinct color bands\"  # Change 'sks' to unique token, describe your subject\n",
        "\n",
        "# Training settings\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "RESOLUTION = 512\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "MAX_TRAIN_STEPS = 1000  # More steps = better quality but longer training\n",
        "LEARNING_RATE = 1e-4\n",
        "LORA_RANK = 4  # Higher = more capacity but more VRAM (4-128)\n",
        "\n",
        "# ===== PART 3: PREPARE DATASET =====\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Create working directory\n",
        "WORK_DIR = \"/content/training_data\"\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "# Copy and resize images from your Drive\n",
        "def prepare_dataset(source_path, dest_path, size=512):\n",
        "    os.makedirs(dest_path, exist_ok=True)\n",
        "    valid_ext = ('.jpg', '.jpeg', '.png', '.webp')\n",
        "    count = 0\n",
        "\n",
        "    for f in os.listdir(source_path):\n",
        "        if f.lower().endswith(valid_ext):\n",
        "            img_path = os.path.join(source_path, f)\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize((size, size), Image.LANCZOS)\n",
        "                img.save(os.path.join(dest_path, f\"{count:04d}.png\"))\n",
        "                count += 1\n",
        "                print(f\"Processed: {f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Skipped {f}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Prepared {count} images for training\")\n",
        "    return count\n",
        "\n",
        "num_images = prepare_dataset(DATASET_PATH, WORK_DIR, RESOLUTION)\n",
        "\n",
        "# ===== PART 4: LORA TRAINING =====\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
        "from diffusers.loaders import LoraLoaderMixin\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Custom Dataset\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(self, data_dir, prompt, tokenizer, size=512):\n",
        "        self.data_dir = data_dir\n",
        "        self.prompt = prompt\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "        self.images = [f for f in os.listdir(data_dir) if f.endswith('.png')]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((size, size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.data_dir, self.images[idx])).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=77,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\"pixel_values\": img, \"input_ids\": tokens.input_ids.squeeze()}\n",
        "\n",
        "# Load model components\n",
        "print(\"Loading model...\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder\n",
        "vae = pipe.vae\n",
        "unet = pipe.unet\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "\n",
        "# Freeze VAE and text encoder\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Apply LoRA to UNet\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_RANK,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
        ")\n",
        "\n",
        "unet = get_peft_model(unet, lora_config)\n",
        "unet.print_trainable_parameters()\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = DreamBoothDataset(WORK_DIR, INSTANCE_PROMPT, tokenizer, RESOLUTION)\n",
        "dataloader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "print(f\"\\nüöÄ Starting LoRA training for {MAX_TRAIN_STEPS} steps...\")\n",
        "unet.train()\n",
        "global_step = 0\n",
        "losses = []\n",
        "\n",
        "while global_step < MAX_TRAIN_STEPS:\n",
        "    for batch in dataloader:\n",
        "        if global_step >= MAX_TRAIN_STEPS:\n",
        "            break\n",
        "\n",
        "        pixel_values = batch[\"pixel_values\"].to(\"cuda\", dtype=torch.float16)\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "\n",
        "        # Encode images to latent space\n",
        "        latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
        "\n",
        "        # Sample noise\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=\"cuda\").long()\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get text embeddings\n",
        "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            avg_loss = sum(losses[-50:]) / len(losses[-50:])\n",
        "            print(f\"Step {global_step}/{MAX_TRAIN_STEPS} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete!\")\n",
        "\n",
        "# ===== PART 5: SAVE LORA WEIGHTS =====\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "unet.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"‚úÖ LoRA weights saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# ===== PART 6: GENERATE IMAGES WITH YOUR LORA =====\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load base model and apply your LoRA\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Load LoRA weights\n",
        "pipe.load_lora_weights(OUTPUT_DIR)\n",
        "\n",
        "def generate_with_lora(prompt, num_images=1, steps=50, guidance=7.5):\n",
        "    \"\"\"Generate images using your trained LoRA\"\"\"\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        num_inference_steps=steps,\n",
        "        guidance_scale=guidance,\n",
        "        num_images_per_prompt=num_images\n",
        "    ).images\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
        "    if len(images) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, img in zip(axes, images):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.suptitle(prompt[:60] + \"...\" if len(prompt) > 60 else prompt)\n",
        "    plt.show()\n",
        "\n",
        "    return images\n",
        "\n",
        "# Test your LoRA!\n",
        "# Use the same token you used in INSTANCE_PROMPT\n",
        "images = generate_with_lora(f\"{INSTANCE_PROMPT} in a beautiful garden\", num_images=2)\n",
        "\n",
        "# ===== PART 7: ANIMATE TO VIDEO =====\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def animate_image_to_video(image, output_path=\"animated_output.mp4\", duration=5, effect=\"zoom\"):\n",
        "    \"\"\"Animate image: zoom, zoom_out, pan_left, pan_right, pulse, rotate\"\"\"\n",
        "    img_array = np.array(image)\n",
        "    fps = 30\n",
        "    total_frames = duration * fps\n",
        "    frames = []\n",
        "    h, w = img_array.shape[:2]\n",
        "\n",
        "    if effect == \"zoom\":\n",
        "        for i in range(total_frames):\n",
        "            scale = 1 + (i / total_frames) * 0.3\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            resized = cv2.resize(img_array, (new_w, new_h))\n",
        "            start_y, start_x = (new_h - h) // 2, (new_w - w) // 2\n",
        "            frames.append(resized[start_y:start_y+h, start_x:start_x+w])\n",
        "    elif effect == \"zoom_out\":\n",
        "        for i in range(total_frames):\n",
        "            scale = 1.3 - (i / total_frames) * 0.3\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            resized = cv2.resize(img_array, (new_w, new_h))\n",
        "            start_y, start_x = (new_h - h) // 2, (new_w - w) // 2\n",
        "            frames.append(resized[start_y:start_y+h, start_x:start_x+w])\n",
        "    elif effect == \"pan_left\":\n",
        "        padded = cv2.copyMakeBorder(img_array, 0, 0, w//4, w//4, cv2.BORDER_REFLECT)\n",
        "        for i in range(total_frames):\n",
        "            offset = int((i / total_frames) * (w // 2))\n",
        "            frames.append(padded[:, offset:offset+w])\n",
        "    elif effect == \"pan_right\":\n",
        "        padded = cv2.copyMakeBorder(img_array, 0, 0, w//4, w//4, cv2.BORDER_REFLECT)\n",
        "        for i in range(total_frames):\n",
        "            offset = int(((total_frames - i) / total_frames) * (w // 2))\n",
        "            frames.append(padded[:, offset:offset+w])\n",
        "    elif effect == \"pulse\":\n",
        "        for i in range(total_frames):\n",
        "            scale = 1 + 0.05 * np.sin(2 * np.pi * i / (fps * 1.5))\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            resized = cv2.resize(img_array, (new_w, new_h))\n",
        "            start_y, start_x = (new_h - h) // 2, (new_w - w) // 2\n",
        "            cropped = resized[max(0,start_y):start_y+h, max(0,start_x):start_x+w]\n",
        "            frames.append(cv2.resize(cropped, (w, h)) if cropped.shape[:2] != (h,w) else cropped)\n",
        "    elif effect == \"rotate\":\n",
        "        for i in range(total_frames):\n",
        "            angle = (i / total_frames) * 10 - 5\n",
        "            matrix = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.1)\n",
        "            frames.append(cv2.warpAffine(img_array, matrix, (w, h), borderMode=cv2.BORDER_REFLECT))\n",
        "\n",
        "    clip = ImageSequenceClip(frames, fps=fps)\n",
        "    clip.write_videofile(output_path, codec='libx264', audio=False, verbose=False, logger=None)\n",
        "    print(f\"‚úÖ Video saved: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def show_video(path):\n",
        "    mp4 = open(path, 'rb').read()\n",
        "    return HTML(f'<video width=512 controls><source src=\"data:video/mp4;base64,{b64encode(mp4).decode()}\" type=\"video/mp4\"></video>')\n",
        "\n",
        "# Animate your generated image\n",
        "if images:\n",
        "    video_path = animate_image_to_video(images[0], effect=\"zoom\", duration=5)\n",
        "    show_video(video_path)\n",
        "\n",
        "# ===== QUICK REFERENCE =====\n",
        "#\n",
        "# 1. Put 10-20 images of your subject in a Google Drive folder\n",
        "# 2. Update DATASET_PATH to point to that folder\n",
        "# 3. Change INSTANCE_PROMPT (use unique token like \"sks\" + description)\n",
        "# 4. Run all cells and wait for training (~15-30 mins)\n",
        "# 5. Generate with: generate_with_lora(\"a photo of sks person as an astronaut\")\n",
        "# 6. Animate with: animate_image_to_video(images[0], effect=\"zoom\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833,
          "referenced_widgets": [
            "ee222c952c78416993009483ddba8c73",
            "475447bda55b409880aeb59e0d8d8d46",
            "f521b0efc471489b8377b489b0ce510c",
            "cf03d8386723458eb622d7df25b47a33",
            "47d34d127fc6415d83faa9b6dd429005",
            "501a611b0ac64e11a9771dbc404d7e23",
            "87fed1be2e7e421aa065ebc644b96e8a",
            "8e0e55919caf4961828584594a08cb43",
            "e92aab6f3a924fe1867d03ecdc9baef9",
            "cbae7db0217341649958a63558145a74",
            "ff6a724d4da746aab59652042a52da1d"
          ]
        },
        "id": "5mjooBPNEpiE",
        "outputId": "af92d603-fb62-46c9-c3c1-902326a1d4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed: img_0005.jpg\n",
            "Processed: img_0012.jpg\n",
            "Processed: img_0008.jpg\n",
            "Processed: img_0004.jpg\n",
            "Processed: img_0014.jpg\n",
            "Processed: img_0006.jpg\n",
            "Processed: img_0001.jpg\n",
            "Processed: img_0011.jpg\n",
            "Processed: img_0013.jpg\n",
            "Processed: img_0009.jpg\n",
            "Processed: img_0000.jpg\n",
            "Processed: img_0015.jpg\n",
            "Processed: img_0010.jpg\n",
            "Processed: img_0002.jpg\n",
            "Processed: img_0007.jpg\n",
            "Processed: img_0003.jpg\n",
            "\n",
            "‚úÖ Prepared 16 images for training\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee222c952c78416993009483ddba8c73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-793244178.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0msafety_checker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m )\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_in_4bit_bnb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_in_8bit_bnb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_group_offloaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4341\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m                 )\n\u001b[0;32m-> 4343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Note: pytubefix import removed to avoid bot detection issues\n",
        "# We extract video IDs using regex instead\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "import tempfile\n",
        "import whisper\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_info = f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\" if device.type == \"cuda\" else \"‚ö†Ô∏è GPU not found! Running on CPU (slow).\"\n",
        "print(device_info)\n",
        "\n",
        "# LED model for long documents\n",
        "print(\"Loading LED model...\")\n",
        "MODEL_NAME = \"allenai/led-base-16384\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).half().to(device).eval()\n",
        "MAX_INPUT_LEN = 16384\n",
        "print(\"LED model loaded!\")\n",
        "\n",
        "TRANSCRIPT_FILE = \"/content/transcripts.txt\"\n",
        "\n",
        "# OpenRouter API configuration\n",
        "try:\n",
        "    OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "except:\n",
        "    OPENROUTER_API_KEY = None\n",
        "    print(\"‚ö†Ô∏è OpenRouter API key not found. Topic extraction will be disabled.\")\n",
        "    print(\"To enable: Click the üîë icon in left sidebar ‚Üí Add secret named 'OPENROUTER_API_KEY'\")\n",
        "\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "# Whisper model cache\n",
        "whisper_model = None\n",
        "\n",
        "def get_whisper_model(model_name=\"base\"):\n",
        "    global whisper_model\n",
        "    if whisper_model is None:\n",
        "        print(f\"Loading Whisper {model_name} model...\")\n",
        "        whisper_model = whisper.load_model(model_name)\n",
        "        print(\"Whisper model loaded!\")\n",
        "    return whisper_model\n",
        "\n",
        "# --- YouTube Utilities ---\n",
        "def get_youtube_id(url: str) -> str:\n",
        "    \"\"\"Extract video ID from YouTube URL without using pytubefix (avoids bot detection).\"\"\"\n",
        "    import re\n",
        "    patterns = [\n",
        "        r'(?:youtube\\.com\\/watch\\?v=|youtu\\.be\\/|youtube\\.com\\/embed\\/|youtube\\.com\\/v\\/)([a-zA-Z0-9_-]{11})',\n",
        "        r'(?:youtube\\.com\\/shorts\\/)([a-zA-Z0-9_-]{11})',\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    raise ValueError(f\"Could not extract video ID from URL: {url}\")\n",
        "\n",
        "def yt_transcribe(video_id):\n",
        "    try:\n",
        "        subt = YouTubeTranscriptApi().fetch(video_id)\n",
        "        text_parts = [item.text for item in subt]\n",
        "        return \" \".join(text_parts)\n",
        "    except Exception as e:\n",
        "        print(f\"Transcript not available: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Video / Audio Utilities ---\n",
        "def repair_video(input_path):\n",
        "    fixed_path = input_path.replace(\".mp4\", \"_fixed.mp4\")\n",
        "    subprocess.run(\n",
        "        [\"ffmpeg\", \"-y\", \"-i\", input_path, \"-c\", \"copy\", \"-movflags\", \"faststart\", fixed_path],\n",
        "        check=True,\n",
        "        capture_output=True\n",
        "    )\n",
        "    return fixed_path\n",
        "\n",
        "def extract_audio_ffmpeg(video_path):\n",
        "    audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
        "        \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
        "        audio_path\n",
        "    ], check=True, capture_output=True)\n",
        "    return audio_path\n",
        "\n",
        "def transcribe_video(video_path, model_name=\"base\"):\n",
        "    model = get_whisper_model(model_name)\n",
        "    audio_path = extract_audio_ffmpeg(video_path)\n",
        "    try:\n",
        "        result = model.transcribe(audio_path)\n",
        "        return result[\"text\"]\n",
        "    finally:\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "\n",
        "# --- Simplified Video Generation (prompt only) ---\n",
        "def generate_video_from_prompt(prompt):\n",
        "    \"\"\"Generate image and animate it to video with fixed default parameters.\"\"\"\n",
        "    if not prompt or not prompt.strip():\n",
        "        return None, \"‚ùå Please enter a prompt.\"\n",
        "\n",
        "    try:\n",
        "        # Fixed default parameters\n",
        "        num_images = 1\n",
        "        steps = 50\n",
        "        guidance = 7.5\n",
        "        effect = \"zoom\"\n",
        "        duration = 5\n",
        "\n",
        "        # Generate image with fixed parameters\n",
        "        images = generate_with_lora(prompt, num_images=num_images, steps=steps, guidance=guidance)\n",
        "        image = images[0]\n",
        "\n",
        "        # Animate to video with fixed parameters\n",
        "        output_path = \"/content/animated_video.mp4\"\n",
        "        video_path = animate_image_to_video(image, output_path=output_path, effect=effect, duration=duration)\n",
        "\n",
        "        return video_path, \"‚úÖ Video generated successfully!\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Error generating video: {e}\"\n",
        "\n",
        "# --- File Utilities ---\n",
        "def save_transcript_to_file(text, filename=TRANSCRIPT_FILE):\n",
        "    with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(text + \"\\n\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "# --- Summarization Utilities ---\n",
        "def chunk_text(text, max_tokens=MAX_INPUT_LEN):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    cur_chunk = []\n",
        "    cur_len = 0\n",
        "    for word in words:\n",
        "        cur_len += len(tokenizer.encode(word, add_special_tokens=False))\n",
        "        cur_chunk.append(word)\n",
        "        if cur_len >= max_tokens:\n",
        "            chunks.append(\" \".join(cur_chunk))\n",
        "            cur_chunk = []\n",
        "            cur_len = 0\n",
        "    if cur_chunk:\n",
        "        chunks.append(\" \".join(cur_chunk))\n",
        "    return chunks\n",
        "\n",
        "def summarize_chunk(text, min_len=250, max_len=450):\n",
        "    enc = tokenizer(text, padding=True, truncation=True, max_length=MAX_INPUT_LEN, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            **enc,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            num_beams=4,\n",
        "            no_repeat_ngram_size=3,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    return tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "def summarize_large_text(text, min_len=250, max_len=450):\n",
        "    chunks = chunk_text(text)\n",
        "    chunk_summaries = [summarize_chunk(c, min_len, max_len) for c in chunks]\n",
        "    combined_summary = \" \".join(chunk_summaries)\n",
        "    final_summary = summarize_chunk(combined_summary, min_len, max_len)\n",
        "    return final_summary\n",
        "\n",
        "# --- Topic Extraction with OpenRouter Mistral ---\n",
        "def extract_main_topics(transcript_text):\n",
        "    if not OPENROUTER_API_KEY:\n",
        "        return \"‚ö†Ô∏è OpenRouter API key not found. Please add it to Colab Secrets (üîë icon in sidebar).\"\n",
        "\n",
        "    try:\n",
        "        max_chars = 15000\n",
        "        truncated_text = transcript_text[:max_chars] if len(transcript_text) > max_chars else transcript_text\n",
        "\n",
        "        prompt = f\"\"\"Analyze the following educational content transcript and identify exactly 3 main topics discussed.\n",
        "For each topic, provide:\n",
        "1. A clear, concise topic title (5-10 words)\n",
        "2. A brief description (1-2 sentences)\n",
        "\n",
        "Format your response as:\n",
        "**Topic 1: [Title]**\n",
        "[Description]\n",
        "\n",
        "**Topic 2: [Title]**\n",
        "[Description]\n",
        "\n",
        "**Topic 3: [Title]**\n",
        "[Description]\n",
        "\n",
        "Transcript:\n",
        "{truncated_text}\n",
        "\"\"\"\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"mistralai/mistral-7b-instruct:free\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"max_tokens\": 800,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "\n",
        "        response = requests.post(OPENROUTER_URL, headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        topics = result['choices'][0]['message']['content']\n",
        "        return topics\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error extracting topics: {e}\"\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_videos(youtube_links, uploaded_files, progress=gr.Progress()):\n",
        "    transcripts = []\n",
        "    errors = []\n",
        "\n",
        "    open(TRANSCRIPT_FILE, \"w\").close()\n",
        "\n",
        "    if youtube_links and youtube_links.strip():\n",
        "        links = [link.strip() for link in youtube_links.strip().splitlines() if link.strip()]\n",
        "        for i, link in enumerate(links):\n",
        "            progress((i + 1) / (len(links) + 1), desc=f\"Processing YouTube link {i + 1}/{len(links)}\")\n",
        "            try:\n",
        "                video_id = get_youtube_id(link)\n",
        "                transcript = yt_transcribe(video_id)\n",
        "                if transcript:\n",
        "                    save_transcript_to_file(f\"Transcript for {link}:\\n{transcript}\")\n",
        "                    transcripts.append(transcript)\n",
        "                else:\n",
        "                    errors.append(f\"‚ö†Ô∏è Could not get transcript for {link}\")\n",
        "            except Exception as e:\n",
        "                errors.append(f\"‚ùå Error processing {link}: {e}\")\n",
        "\n",
        "    if uploaded_files:\n",
        "        for i, uploaded_file in enumerate(uploaded_files):\n",
        "            progress((i + 1) / (len(uploaded_files) + 1), desc=f\"Transcribing video {i + 1}/{len(uploaded_files)}\")\n",
        "            try:\n",
        "                if hasattr(uploaded_file, 'name'):\n",
        "                    file_path = uploaded_file.name\n",
        "                else:\n",
        "                    file_path = uploaded_file\n",
        "\n",
        "                fixed_video = repair_video(file_path)\n",
        "                transcript = transcribe_video(fixed_video, model_name=\"base\")\n",
        "                if transcript:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    save_transcript_to_file(f\"Transcript for {filename}:\\n{transcript}\")\n",
        "                    transcripts.append(transcript)\n",
        "            except Exception as e:\n",
        "                errors.append(f\"‚ùå Error processing uploaded file: {e}\")\n",
        "\n",
        "    if not transcripts:\n",
        "        error_msg = \"\\n\".join(errors) if errors else \"No transcripts were generated.\"\n",
        "        return \"\", \"\", \"\", error_msg\n",
        "\n",
        "    combined_text = \"\\n\\n\".join(transcripts)\n",
        "\n",
        "    progress(0.7, desc=\"Extracting main topics...\")\n",
        "    topics = extract_main_topics(combined_text)\n",
        "\n",
        "    progress(0.9, desc=\"Generating summary...\")\n",
        "    summarized_text = summarize_large_text(combined_text, min_len=250, max_len=450)\n",
        "\n",
        "    error_msg = \"\\n\".join(errors) if errors else \"‚úÖ Processing completed successfully!\"\n",
        "\n",
        "    return combined_text, topics, summarized_text, error_msg\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "with gr.Blocks(\n",
        "    title=\"Educational Content Summarizer\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".gradio-container {max-width: 1200px !important}\"\n",
        ") as demo:\n",
        "    gr.Markdown(\"# üìö Educational Content Creation through Multi-Video Summarization\")\n",
        "    gr.Markdown(f\"**Device Status:** {device_info}\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            youtube_input = gr.Textbox(\n",
        "                label=\"üîó YouTube Video Links\",\n",
        "                placeholder=\"Enter YouTube links (one per line)...\\n\\nExample:\\nhttps://www.youtube.com/watch?v=VIDEO_ID\",\n",
        "                lines=5\n",
        "            )\n",
        "            video_upload = gr.File(\n",
        "                label=\"üìÅ Upload Video Files\",\n",
        "                file_types=[\".mp4\", \".avi\", \".mov\", \".mkv\"],\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "            process_btn = gr.Button(\"üé¨ Get Detailed Notes\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"üìù Transcript\"):\n",
        "                    transcript_output = gr.Textbox(\n",
        "                        label=\"Combined Transcript\",\n",
        "                        lines=15,\n",
        "                        show_copy_button=True\n",
        "                    )\n",
        "                with gr.TabItem(\"üéØ Main Topics\"):\n",
        "                    topics_output = gr.Markdown(label=\"Main Topics Identified\")\n",
        "                with gr.TabItem(\"üìã Summary\"):\n",
        "                    summary_output = gr.Textbox(\n",
        "                        label=\"Final Summary (~20 sentences)\",\n",
        "                        lines=10,\n",
        "                        show_copy_button=True\n",
        "                    )\n",
        "                with gr.TabItem(\"üé® Video Generator\"):\n",
        "                    gr.Markdown(\"## üé• Generate Animated Video from Prompt\")\n",
        "                    gr.Markdown(\"*Enter a prompt and get an animated video automatically.*\")\n",
        "\n",
        "                    prompt_input = gr.Textbox(\n",
        "                        label=\"üñäÔ∏è Prompt\",\n",
        "                        placeholder=\"Describe the scene to generate...\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    generate_video_btn = gr.Button(\"üé¨ Generate Video\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                    video_output = gr.Video(label=\"Generated Video\")\n",
        "                    video_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    generate_video_btn.click(\n",
        "                        fn=generate_video_from_prompt,\n",
        "                        inputs=[prompt_input],\n",
        "                        outputs=[video_output, video_status]\n",
        "                    )\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Status\",\n",
        "                lines=3,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=process_videos,\n",
        "        inputs=[youtube_input, video_upload],\n",
        "        outputs=[transcript_output, topics_output, summary_output, status_output],\n",
        "        show_progress=\"full\"\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### üìñ Instructions:\n",
        "    1. **YouTube Videos**: Paste YouTube video links (one per line)\n",
        "    2. **Local Videos**: Upload video files (.mp4, .avi, .mov, .mkv)\n",
        "    3. Click **\"Get Detailed Notes\"** to process\n",
        "    4. View results in the tabs: Transcript, Topics, and Summary\n",
        "    5. **Video Generator**: Enter a prompt and click \"Generate Video\" to create an animated video\n",
        "\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "Htx1ZIfAEu-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(\n",
        "    share=True,  # Creates a public URL you can share\n",
        "    debug=True,  # Shows detailed errors\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "id": "l0Ya1D8rEyOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}